# 題目一  
 #### 過往的經驗我們會經由壓力測試評估系統承載能力，並且依照服務預估的規模來提前加開服務並擴展 infra，將服務分散在獨立的 server 來確保負載均衡，確保服務不會因為承受不起 peak loads 而崩潰。流量入站後，密切關注各項監控指標，及時發現和處理潛在問題。

A. 預先加開並且善用自動擴展  
- 在網頁未正式上線前先行評估服務所需效能，提前加開機器避免流量失衡。  
- 可參考類似的活動頁面效能，平日的 1/100 流量所需要的效能或進行壓測取得，來推估需要擴展的機器數量。  
- 以 K8s 舉例，推估 node 需要多幾台，pod 利用污點與親和性來平均放置在擴展的 node 上，並開啟 Kubernetes 自動擴展功能善加利用。  

B. 善加利用 Cache 減輕系統負擔  
- 靜態檔案考慮 CDN。  
- APP 經常讀取資料考慮使用 Redis 來提高 APP 運作效能。  

C. 完善設置監控  
- 密切監控各項基礎數據，確保 CPU、內存、磁盤 I/O、網路流量等資源使用情況，確保沒有資源瓶頸。  
- 監控 Redis 與 CDN 的擊中率可以大幅降低系統壓力，及時發現和處理潛在問題。  
---

# 題目二  
 #### 在只有其中一台發生問題的情況下，以往的經驗告訴我比較有可能是系統效能問題，可能是流量失衡沒有平均分配，或是程式有不正常請求。首先我會先從 **A** 步驟開始排查。

 A. 檢查系統資源使用情況  
- 到 Grafana 檢查 CPU、內存、磁盤 I/O、網路流量等資源使用情況，確保沒有資源瓶頸。  
- 如沒有完善的監控，可使用工具如 `top`、`free`、`iostat`、`df` 來確認系統狀況。  

 B. 應用程式異常  
- 該機器上的應用程式可能出現記憶體洩漏、線程死鎖或其他異常，導致回應時間延長。  
- 是否收到異常請求？如果是，則進入 **C** 步驟，並檢查 WAF 設定。  

 C. 檢查應用日誌和錯誤日誌，不排除是惡意攻擊（DoS 或 DDoS）  
- 使用集中式日誌管理工具（如 ELK/EFK）來分析日誌。  
---

# 題目三  
#### 通常雲端會有其他方式可以登入機器，我會首先嘗試。如果仍無法登入，不排除機器死當狀況，會嘗試重啟。也有遇過 sshd 程序崩潰的狀態，比較常見的錯誤可能是 SSH key 設定錯誤或已被更換，所以我會依照以下順序排除。

A. 與組員確認是否遭遇相同狀況，如果他能登入，確定 SSH key 是否已經更換。

B. 使用 EC2 系統管理員 (SSM) 進行連接
   - 使用 AWS Systems Manager (SSM) 來連接到 EC2 實例，進行故障排查。
   - 確保 EC2 實例已安裝並配置 SSM 代理。

C. 如果仍無法解決，只能嘗試重啟機器，懷疑是 sshd 掛了。
---

# 題目四  
#### 在配置 Filebeat 或 Fluentd 的過程中，注意日誌格式是否與既有的 index 一致，或是根據業務相關性來考量是否放入既有的 index。如果是 K8s，通常是統一的日誌規格，透過標準的日誌收集器即可收集；若是單獨的 VM 服務，則需特別注意日誌規格，以免污染原有的 index。如果建立新的 index，需注意命名規則以方便管理，並設定日誌 retention 的週期，再設置冷儲存週期以降低日誌存儲成本。

 A. 配置 Elasticsearch 索引  
- 在 Elasticsearch 中配置新的索引模式，以適應新服務的日誌結構。  

 B. 配置日誌收集器  
- 配置 Filebeat 或 Fluentd 作為日誌收集器，將新服務的日誌發送到 Elasticsearch。 

 C. 配置 Kibana 儀表板  
- 在 Kibana 中創建新的儀表板和視圖，方便開發者進行日誌查詢和錯誤排查。  
- 配置適當的過濾器和查詢，確保日誌數據能夠快速檢索和分析。  
- 基於日誌配置 alert，有助於推進白盒監控。  